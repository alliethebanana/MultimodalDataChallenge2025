{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6299f767",
   "metadata": {},
   "source": [
    "## How This Geospatial Encoding Works\n",
    "\n",
    "This notebook implements a **hierarchical geospatial encoder** that converts geographic coordinates into learnable embeddings. Here's how each component works:\n",
    "\n",
    "### 1. `emb_dim(n)` Function\n",
    "- Calculates embedding dimension based on vocabulary size\n",
    "- Formula: `min(64, max(8, round(2*sqrt(n))))`\n",
    "- Ensures reasonable embedding sizes (8-64 dimensions) \n",
    "- Larger vocabularies get bigger embeddings, but capped at 64\n",
    "\n",
    "### 2. `HierGeoEncoder` Class\n",
    "This is the main encoder that processes **multi-level geohash data**:\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Geohashes** are strings that encode lat/lng coordinates (e.g., \"9q8y\" → San Francisco area)\n",
    "- **Hierarchical levels** use different precisions: \"9\" → \"9q\" → \"9q8\" → \"9q8y\" (coarse → fine)\n",
    "- Each level captures geographic information at different scales\n",
    "\n",
    "**Architecture:**\n",
    "- Creates separate embedding layers for each geohash level\n",
    "- Combines all embeddings + optional extra features (like seasonal data)\n",
    "- Passes through MLP to create final geographic representation\n",
    "\n",
    "### 3. `GeoLateHead` Class\n",
    "- Takes the geographic encoder and adds a classification head\n",
    "- Maps geographic embeddings to class predictions (183 classes for fungi species)\n",
    "- Useful for predicting species based on geographic location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d2e7b",
   "metadata": {},
   "source": [
    "## Forward Pass Example\n",
    "\n",
    "Let's say you have a sample with geohash \"9q8yv\" and want to encode it hierarchically:\n",
    "\n",
    "```\n",
    "Input: \n",
    "- level_ids = [tensor([9]), tensor([9q]), tensor([9q8]), tensor([9q8y]), tensor([9q8yv])]\n",
    "- extra = tensor([0.5, -0.8])  # e.g., month_sin, month_cos\n",
    "\n",
    "Process:\n",
    "1. Each embedding layer processes its level:\n",
    "   - emb[0](9) → 16-dim vector\n",
    "   - emb[1](9q) → 20-dim vector  \n",
    "   - emb[2](9q8) → 24-dim vector\n",
    "   - etc.\n",
    "\n",
    "2. Concatenate all embeddings: [16 + 20 + 24 + ... + extra_dims]\n",
    "\n",
    "3. Pass through MLP: \n",
    "   - Linear(total_dims → 256) → ReLU → Dropout\n",
    "   - Linear(256 → 128) → ReLU → Dropout\n",
    "   - Output: 128-dim geographic representation\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "- **Multi-scale learning**: Captures both broad regions and precise locations\n",
    "- **Learnable representations**: Neural network learns optimal geographic features\n",
    "- **Flexible**: Can add temporal/seasonal information via `extra` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a41876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, math\n",
    "\n",
    "def emb_dim(n):  \n",
    "    return int(min(64, max(8, round(2*math.sqrt(n)))))\n",
    "\n",
    "class HierGeoEncoder(nn.Module):\n",
    "    def __init__(self, vocab_levels, num_extra=0, out_dim=128, dropout=0.2):\n",
    "        \"\"\"\n",
    "        vocab_levels: list of dicts (level vocab), one per precision (e.g., geohash[:3], [:4], ...)\n",
    "        num_extra: extra numeric dims (e.g., month_sin, month_cos)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(len(v), emb_dim(len(v)), padding_idx=0) for v in vocab_levels\n",
    "        ])\n",
    "        cat_dim = sum(emb.embedding_dim for emb in self.embs)\n",
    "        self.num_extra = num_extra\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cat_dim + num_extra, 256), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(256, out_dim), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, level_ids, extra=None):\n",
    "        # level_ids: list of LongTensors [N] (one per level)\n",
    "        zs = [emb(ids) for emb, ids in zip(self.embs, level_ids)]   # [(N,d_i)...]\n",
    "        z = torch.cat(zs, dim=-1)                                   # (N, sum d_i)\n",
    "        if self.num_extra and extra is not None:\n",
    "            z = torch.cat([z, extra], dim=-1)\n",
    "        return self.mlp(z)                                          # (N, out_dim)\n",
    "\n",
    "class GeoLateHead(nn.Module):\n",
    "    def __init__(self, geo_enc: HierGeoEncoder, n_classes=183):\n",
    "        super().__init__()\n",
    "        self.geo = geo_enc\n",
    "        self.clf = nn.Linear(geo_enc.mlp[-2].out_features, n_classes)\n",
    "    def forward(self, level_ids, extra=None):\n",
    "        z = self.geo(level_ids, extra)      # (N, D)\n",
    "        return self.clf(z)                  # (N, C) metadata logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c2992",
   "metadata": {},
   "source": [
    "v2 with date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657435d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
